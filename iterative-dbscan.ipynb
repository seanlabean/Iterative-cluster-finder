{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from amuse.lab import generic_unit_converter, nbody_system\n",
    "from amuse.lab import units as u\n",
    "from amuse.lab import Particles\n",
    "import amuse.lab\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 24}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc({'savefig.dpi':300})\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import sys\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters_with_dbscan(stars, outer_density_limit=1.0 | u.MSun*u.parsec**-3,\n",
    "                              avg_stellar_mass=0.586,\n",
    "                              eps=0.4, min_samples=12, leaf_size=30,\n",
    "                              return_labels=False, debug=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Find all the stars in clusters using\n",
    "    the DBSCAN implementation from scikit-learn.\n",
    "\n",
    "    Keyword Arguments:\n",
    "    stars               -- AMUSE particle set\n",
    "    outer_density_limit -- If set, use this density_limit\n",
    "                           in solar masses per parsec^-3 to\n",
    "                           compute eps to figure\n",
    "                           out where the cluster edges are\n",
    "                           instead of the input eps. \n",
    "                           A good default choice\n",
    "                           is 1.0 MSun * pc^-3.\n",
    "                           Note this setting overrides eps.\n",
    "    avg_stellar_mass    -- Average stellar mass of the IMF\n",
    "                           used to make the stars your\n",
    "                           clustering. Default is \n",
    "                           from a Kroupa IMF that goes \n",
    "                           from 0.08 to 150 Msun.\n",
    "    eps                 -- Minimum  neighbor distance to be\n",
    "                           considered in the cluster in pc.\n",
    "                           This value is calculated for you\n",
    "                           if you use outer_density_limit.\n",
    "    min_samples         -- Minimum number of neighbors to\n",
    "                           be considered a core particle.\n",
    "                           Default is 12, the default for\n",
    "                           DBSCAN.\n",
    "    leaf_size           -- Number of particles in a leaf\n",
    "                           on the KD tree the code uses\n",
    "                           to find neighbors. Default is\n",
    "                           30, the default for DBSCAN.\n",
    "    return_labels       -- Also return the raw DBSCAN label output?\n",
    "    debug               -- Turn on debugging output\n",
    "\n",
    "    Returns:\n",
    "    groups        -- A list of particle sets for the particles in each cluster.\n",
    "    n_groups      -- The number of clusters found.\n",
    "    labels        -- The actual label indicies returned by DBSCAN,\n",
    "                     only returned if return_labels=True.\n",
    "    unique_labels -- The unique labels returned by DBSCAN,\n",
    "                     only returned if return_labels=True.\n",
    "    \"\"\"\n",
    "\n",
    "    pre = \"[find_clusters_with_dbscan]:\"\n",
    "\n",
    "    if (outer_density_limit is not None):\n",
    "\n",
    "        # The number of samples should\n",
    "        # be greater than that of\n",
    "        # the average density of the\n",
    "        # SN in a pc^3 (~ 0.01, BT), but not as high\n",
    "        # as that of an open cluster \n",
    "        # (~10 Msun / pc^3, Binney and Tremaine)\n",
    "\n",
    "        # Note the mean number density of the solar\n",
    "        # neighborhood is 0.17 stars per parsec^3,\n",
    "        # while the mean in an open cluster is 17 stars\n",
    "        # per parsec^3, so a good choice is the mean\n",
    "        # of these in log space, or about 1 star per parsec^-3.\n",
    "\n",
    "        # So here we are saying they should be at least closer\n",
    "        # that the average distance between stars in the SN.\n",
    "        number_density_limit = (outer_density_limit.value_in(u.MSun*u.parsec**-3) \n",
    "                             / avg_stellar_mass)\n",
    "\n",
    "        if (number_density_limit < 0.17):\n",
    "            print(pre, \"WARNING: Your number density limit \\\n",
    "                        at\", number_density_limit, \"pc^-3 \\\n",
    "                        is smaller than that of the solar \\\n",
    "                        neighborhood, which is ~ 0.17 pc^-3!\")\n",
    "\n",
    "        eps = number_density_limit**(-1./3.)\n",
    "\n",
    "    if (debug):\n",
    "        print(pre, \"outer_density_limit  =\", outer_density_limit)\n",
    "        print(pre, \"avg_stellar_mass     =\", avg_stellar_mass)\n",
    "        print(pre, \"number_density_limit =\", number_density_limit)\n",
    "        print(pre, \"eps =\", eps)\n",
    "\n",
    "    particle_positions = stars.position.value_in(u.parsec)\n",
    "\n",
    "    # Note: I don't think its necessary to scale\n",
    "    #       the inputs for DBSCAN when you are\n",
    "    #       using a simple Eulerian metric on 3-d\n",
    "    #       position space, as its just rescaling eps.\n",
    "\n",
    "    # Get a DBSCAN instance running.\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, leaf_size=leaf_size)\n",
    "    # Do the clustering.\n",
    "    clstrs = db.fit_predict(particle_positions)\n",
    "    # Get the unique cluster lables (i.e. the number of clusters).\n",
    "    labels = db.labels_\n",
    "    unique_labels = set(labels) # This returns only the unique ones.\n",
    "    # Anything with an index of -1 is noise.\n",
    "    #n_groups = len(filter((lambda x: x>=0),unique_labels))\n",
    "    n_groups = 0\n",
    "    groups = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        if (label >= 0): # Don't include noise particles here.\n",
    "            groups.append(stars[np.where(labels == label)[0]])\n",
    "\n",
    "    if (debug):\n",
    "        print(pre, \"groups=\", groups)\n",
    "        print(pre, \"n_groups=\", n_groups)\n",
    "        print(pre, \"labels=\", labels)\n",
    "        print(pre, \"unique_labels=\", unique_labels)\n",
    "\n",
    "    if (return_labels):\n",
    "        return groups, n_groups, labels, unique_labels\n",
    "    else:\n",
    "        return groups, n_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/data\" (1 members)>\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('./example_data/L3-50M-2tff_stars.amuse', 'r')\n",
    "list(f.keys())\n",
    "dset = f['data']\n",
    "print(dset)\n",
    "#find_clusters_with_dbscan(file_,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[find_clusters_with_dbscan]: outer_density_limit  = 1.0 MSun * parsec**-3\n",
      "[find_clusters_with_dbscan]: avg_stellar_mass     = 0.586\n",
      "[find_clusters_with_dbscan]: number_density_limit = 1.70648464164\n",
      "[find_clusters_with_dbscan]: eps = 0.83682093912\n",
      "[find_clusters_with_dbscan]: groups= [<amuse.datamodel.particles.ParticlesSubset object at 0x1241395d0>, <amuse.datamodel.particles.ParticlesSubset object at 0x124543cd0>, <amuse.datamodel.particles.ParticlesSubset object at 0x1245431d0>, <amuse.datamodel.particles.ParticlesSubset object at 0x12414ee50>, <amuse.datamodel.particles.ParticlesSubset object at 0x1241431d0>, <amuse.datamodel.particles.ParticlesSubset object at 0x1246dabd0>]\n",
      "[find_clusters_with_dbscan]: n_groups= 0\n",
      "[find_clusters_with_dbscan]: labels= [-1 -1  0 ...,  4  4  4]\n",
      "[find_clusters_with_dbscan]: unique_labels= {0, 1, 2, 3, 4, 5, -1}\n"
     ]
    }
   ],
   "source": [
    "conv = generic_unit_converter.ConvertBetweenGenericAndSiUnits(\n",
    "        1.0 | u.cm, 1.0 | u.g, 1.0 | u.s)\n",
    "stars = amuse.io.read_set_from_file(\"./example_data/L3-50M-2tff_stars.amuse\", format='amuse')\n",
    "groups, n_groups = find_clusters_with_dbscan(stars,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.98503808e+15  -9.76110951e+16   6.49901847e+16]\n"
     ]
    }
   ],
   "source": [
    "print(groups[0].center_of_mass().value_in(u.m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
