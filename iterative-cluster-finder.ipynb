{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to build a robust cluster finding algorithm that produces meaningful results for my computational astrophysics star cluster formation simulations. Typical out-of-the-box cluster finders rely on particle number density, or nearest neighbor criteria. Such techniques work well for observational research in which less information is known about star or galaxy cluster members. \n",
    "\n",
    "What am I interested in?\n",
    "* Analysis on how much material (stars) is in a bound state within the identified clusters\n",
    "* use 'boundedness' as a criteria for cluster identification\n",
    "* fraction of bound stars in system over time\n",
    "\n",
    "What scientific question am I trying to answer with this tool?\n",
    "* Early forming massive stars form looser assosciations of stars rather than allowing for more massive clusters.\n",
    "    * potential profile of cluster\n",
    "    * density profile of cluster\n",
    "\n",
    "    \n",
    "Methods to create this tool:\n",
    "* Have DBSCAN find clusters from all stars and report.\n",
    "* remove stars that have negative energy (considering all other stars and gas). Recalculate star total energies. Remove E_tot > 0 stars, recalculate energies. Repeat until all stars have E_tot < 0. \n",
    "    * This method feels like it might have some issues as the removed stars still will contribute to potential in reality.\n",
    "    * but [Li+2019](https://arxiv.org/pdf/1904.11987.pdf) found that such a recursive technique results in identification of clusters with lagrangiian radii consistently near the 60% mass bin using the conventional cluster identification method.\n",
    "\n",
    "# Vanilla DBSCAN technique\n",
    "DBSCAN ([Wiki](https://en.wikipedia.org/wiki/DBSCAN), [scipy docs](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)) or Density-Based Spatial Clustering of Applications with Noise, is a clustering algorithm originally proposed in 1996 (Ester et al. 1996) which idenitifies data points that are densely clustered (points with many nearest neighbors) and excludes those with few nearest neighbors.\n",
    "\n",
    "DBSCAN can be used to identify star clusters. \n",
    "1. Input all stars in computational domain\n",
    "2. Pass through DBSCAN, report clusters\n",
    "\n",
    "Cons to this method: DBSCAN does not have any knowledge of dynamics. Only looks for spatially separated density peaks, therefore will have a tendency to overestimate the fraction of stars that are truly members of a particular cluster (includes stars in a cluster even if the stars have v > escape velocity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from amuse.lab import generic_unit_converter, nbody_system\n",
    "from amuse.lab import units as u\n",
    "from amuse.lab import Particles\n",
    "import amuse.lab\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 24}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc({'savefig.dpi':300})\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import sys\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters_with_dbscan(stars, outer_density_limit=1.0 | u.MSun*u.parsec**-3,\n",
    "                              avg_stellar_mass=0.586,\n",
    "                              eps=0.4, min_samples=12, leaf_size=30,\n",
    "                              return_labels=False, debug=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Find all the stars in clusters using\n",
    "    the DBSCAN implementation from scikit-learn.\n",
    "\n",
    "    Keyword Arguments:\n",
    "    stars               -- AMUSE particle set\n",
    "    outer_density_limit -- If set, use this density_limit\n",
    "                           in solar masses per parsec^-3 to\n",
    "                           compute eps to figure\n",
    "                           out where the cluster edges are\n",
    "                           instead of the input eps. \n",
    "                           A good default choice\n",
    "                           is 1.0 MSun * pc^-3.\n",
    "                           Note this setting overrides eps.\n",
    "    avg_stellar_mass    -- Average stellar mass of the IMF\n",
    "                           used to make the stars your\n",
    "                           clustering. Default is \n",
    "                           from a Kroupa IMF that goes \n",
    "                           from 0.08 to 150 Msun.\n",
    "    eps                 -- Minimum  neighbor distance to be\n",
    "                           considered in the cluster in pc.\n",
    "                           This value is calculated for you\n",
    "                           if you use outer_density_limit.\n",
    "    min_samples         -- Minimum number of neighbors to\n",
    "                           be considered a core particle.\n",
    "                           Default is 12, the default for\n",
    "                           DBSCAN.\n",
    "    leaf_size           -- Number of particles in a leaf\n",
    "                           on the KD tree the code uses\n",
    "                           to find neighbors. Default is\n",
    "                           30, the default for DBSCAN.\n",
    "    return_labels       -- Also return the raw DBSCAN label output?\n",
    "    debug               -- Turn on debugging output\n",
    "\n",
    "    Returns:\n",
    "    groups        -- A list of particle sets for the particles in each cluster.\n",
    "    n_groups      -- The number of clusters found.\n",
    "    labels        -- The actual label indicies returned by DBSCAN,\n",
    "                     only returned if return_labels=True.\n",
    "    unique_labels -- The unique labels returned by DBSCAN,\n",
    "                     only returned if return_labels=True.\n",
    "    \"\"\"\n",
    "\n",
    "    pre = \"[find_clusters_with_dbscan]:\"\n",
    "\n",
    "    if (outer_density_limit is not None):\n",
    "\n",
    "        # The number of samples should\n",
    "        # be greater than that of\n",
    "        # the average density of the\n",
    "        # SN in a pc^3 (~ 0.01, BT), but not as high\n",
    "        # as that of an open cluster \n",
    "        # (~10 Msun / pc^3, Binney and Tremaine)\n",
    "\n",
    "        # Note the mean number density of the solar\n",
    "        # neighborhood is 0.17 stars per parsec^3,\n",
    "        # while the mean in an open cluster is 17 stars\n",
    "        # per parsec^3, so a good choice is the mean\n",
    "        # of these in log space, or about 1 star per parsec^-3.\n",
    "\n",
    "        # So here we are saying they should be at least closer\n",
    "        # that the average distance between stars in the SN.\n",
    "        number_density_limit = (outer_density_limit.value_in(u.MSun*u.parsec**-3) \n",
    "                             / avg_stellar_mass)\n",
    "\n",
    "        if (number_density_limit < 0.17):\n",
    "            print(pre, \"WARNING: Your number density limit \\\n",
    "                        at\", number_density_limit, \"pc^-3 \\\n",
    "                        is smaller than that of the solar \\\n",
    "                        neighborhood, which is ~ 0.17 pc^-3!\")\n",
    "\n",
    "        eps = number_density_limit**(-1./3.)\n",
    "\n",
    "    if (debug):\n",
    "        print(pre, \"outer_density_limit  =\", outer_density_limit)\n",
    "        print(pre, \"avg_stellar_mass     =\", avg_stellar_mass)\n",
    "        print(pre, \"number_density_limit =\", number_density_limit)\n",
    "        print(pre, \"eps =\", eps)\n",
    "\n",
    "    particle_positions = stars.position.value_in(u.parsec)\n",
    "\n",
    "    # Note: I don't think its necessary to scale\n",
    "    #       the inputs for DBSCAN when you are\n",
    "    #       using a simple Eulerian metric on 3-d\n",
    "    #       position space, as its just rescaling eps.\n",
    "\n",
    "    # Get a DBSCAN instance running.\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, leaf_size=leaf_size)\n",
    "    # Do the clustering.\n",
    "    clstrs = db.fit_predict(particle_positions)\n",
    "    # Get the unique cluster lables (i.e. the number of clusters).\n",
    "    labels = db.labels_\n",
    "    unique_labels = set(labels) # This returns only the unique ones.\n",
    "    # Anything with an index of -1 is noise.\n",
    "    tmp = []\n",
    "    for val in unique_labels:\n",
    "        if val >= 0: \n",
    "            tmp.append(1)\n",
    "    n_groups = len(tmp)\n",
    "    #n_groups = len(filter((lambda x: x>=0),unique_labels))\n",
    "    groups = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        if (label >= 0): # Don't include noise particles here.\n",
    "            groups.append(stars[np.where(labels == label)[0]])\n",
    "\n",
    "    if (debug):\n",
    "        print(pre, \"groups=\", groups)\n",
    "        print(pre, \"n_groups=\", n_groups)\n",
    "        print(pre, \"labels=\", labels)\n",
    "        print(pre, \"unique_labels=\", unique_labels)\n",
    "\n",
    "    if (return_labels):\n",
    "        return groups, n_groups, labels, unique_labels\n",
    "    else:\n",
    "        return groups, n_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('./example_data/DBSCAN_input/L3-50M-2tff_stars.amuse', 'r')\n",
    "list(f.keys())\n",
    "dset = f['data']\n",
    "print(dset)\n",
    "#find_clusters_with_dbscan(file_,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = generic_unit_converter.ConvertBetweenGenericAndSiUnits(\n",
    "        1.0 | u.cm, 1.0 | u.g, 1.0 | u.s)\n",
    "stars = amuse.io.read_set_from_file(\"./example_data/DBSCAN_input/L3v-2tff_stars.amuse\", format='amuse')\n",
    "groups, n_groups = find_clusters_with_dbscan(stars,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(groups))\n",
    "print(len(groups[0]))\n",
    "print(groups[0].center_of_mass().value_in(u.m))\n",
    "for group in groups:\n",
    "    print(group.center_of_mass().value_in(u.m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blah\n",
    "\n",
    "Now we are able to find clusters in any given AMUSE particle set. Now, need to write a function that, when given a star list constituting one or more clusters, will calculate the cluster(s)' center of mass, and the boundedness of each star to those centers of masses. Then, discard all stars that are not bound to any cluster, and return a AMUSE particle set that is once again NOT separated into clusters.\n",
    "\n",
    "Summary of upcomming function:\n",
    "1. input particle lists of clusters\n",
    "2. calculate clusters' centers of mass, center of mass velocity\n",
    "3. calculate boundedness of all stars to the cluster they belong to (and maybe also to other cluster com's?)\n",
    "4. return a single particle set only containing remaining stars (and no separation by cluster).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_com_props(sg):\n",
    "    group_com = []\n",
    "    group_comv = []\n",
    "    tmp_vel_arr = np.zeros(3)\n",
    "    \n",
    "    group_com.append(sg.center_of_mass().value_in(u.m))\n",
    "    tmp_vel_arr[0]  = ((sg.mass*sg.vx).sum() / sg.mass.sum()).value_in(u.m / u.s)\n",
    "    tmp_vel_arr[1]  = ((sg.mass*sg.vy).sum() / sg.mass.sum()).value_in(u.m / u.s)\n",
    "    tmp_vel_arr[2]  = ((sg.mass*sg.vz).sum() / sg.mass.sum()).value_in(u.m / u.s)\n",
    "    group_comv.append(tmp_vel_arr)\n",
    "        \n",
    "    return group_com, group_comv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sg in groups:\n",
    "    c,v = calculate_cluster_com_props(sg)\n",
    "print(type(c[0]))\n",
    "print(groups[0].position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO FIND STAR BOUNDNEDNESS TO CLUSTER COM, LET'S DO ARRAY BROADCASTING! iT SHOUDL WORK, SHOUDL END WITH AN ARRAY OF BOUNDEDNESS VALUES THAT CAN THEN BBE USED TO SELECT FOR NEGATIVE VALUES THEN RETURN THE CORRESPONDING STARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_star_boundedness_to_cluster_com(groups):\n",
    "    G = 6.6743e-11 #m^3 kg^-2 s^-1\n",
    "    total_stars = Particles()\n",
    "    \n",
    "    for sg in groups: # Cycle through each particle set within the list groups\n",
    "        # Extract star positions, masses, velocities\n",
    "        stars_xyz = sg.position.value_in(u.m)\n",
    "        stars_mass = sg.mass.value_in(u.kg).reshape(len(stars_xyz),1)\n",
    "        stars_vel = sg.velocity.value_in(u.m/u.s)\n",
    "        stars_vel_mag = np.sqrt((sg.velocity.value_in(u.m/u.s)**2).sum(axis=1)).reshape(len(stars_xyz),1)\n",
    "        # Get mass of particle set\n",
    "        group_mass = sg.mass.sum().value_in(u.kg)\n",
    "        # Get center of mass and com velocity of the particle set\n",
    "        # and convert into len(stars)x1 array so we can do array broadcasting.\n",
    "        group_com, group_comv = calculate_cluster_com_props(sg)\n",
    "        com_column = np.tile(group_com,(len(stars_xyz),1))\n",
    "        comv_column = np.tile(group_comv,(len(stars_xyz),1))\n",
    "        \n",
    "        # Broadcast arrays to get each particle's velocity relative to com velocity\n",
    "        stars_vel_rel = stars_vel - comv_column\n",
    "        stars_vel_rel_mag = np.sqrt((stars_vel_rel**2).sum(axis=1)).reshape(len(stars_xyz),1)\n",
    "        # Same method for particle distance to com\n",
    "        dist_to_com = np.sqrt(((stars_xyz-com_column)**2).sum(axis=1)).reshape(len(stars_xyz),1)\n",
    "\n",
    "        star_KE = (0.5 * stars_mass * stars_vel_rel_mag**2).reshape(len(stars_xyz),1)\n",
    "        star_PE = (-1.0 * G * group_mass * stars_mass / dist_to_com)\n",
    "        star_TE = star_KE + star_PE\n",
    "        \n",
    "        sg_copy = sg.copy(keep_structure=True)\n",
    "        unbound_star_ind = np.where(star_TE > 0)\n",
    "        for ind in unbound_star_ind:\n",
    "            sg_copy.remove_particle(sg_copy[ind])\n",
    "        print(len(sg),len(sg_copy))\n",
    "        total_stars += sg_copy\n",
    "\n",
    "    return total_stars\n",
    "calculate_star_boundedness_to_cluster_com(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = np.array([[1,1,1], [2,2,2], [3,3,3]])\n",
    "# x = np.array([1,1,1])\n",
    "# dist = np.zeros((len(s),1))\n",
    "# print(dist)\n",
    "# for i,point in enumerate(s):\n",
    "#     dist[i] = np.linalg.norm(x-s[i])\n",
    "# print(dist)\n",
    "x1 = np.tile(x,(len(s),1))\n",
    "\n",
    "dist = np.sqrt(((s-x1)**2).sum(axis=1)).reshape(len(s),1)\n",
    "print(dist)\n",
    "print(groups[0].velocity.value_in(u.m/u.s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative bound-fraction method\n",
    "\n",
    "[Li, H. (2019)](https://arxiv.org/pdf/1904.11987.pdf) reported using an iterative method of removing all stars from the computational domain with a positive energy:\n",
    "1. Calculate total energy of all stars (Li+19 only looks at potentials from other stars since the system has evolved to be gas-less, I will need to consider the grav potential of the gas).\n",
    "2. Remove all stars with positive total energy.\n",
    "3. Repeat 1,2 until all stars have negative energy.\n",
    "\n",
    "It's possible I could modify this method by also removing all gas cells that have positive energy as well.\n",
    "\n",
    "Li+19 found that the iterative method was an appropriate way to determine the fraction of bound stars and was superior than tracking the lagrangiian radius over time for the entitre computational domain.\n",
    "\n",
    "What I want to do:\n",
    "\n",
    "I want to perform the iterative bound fraction method on my 2tff data outputs to compare bound fractions of stars at that point in time. If the method is quick, I'd like to do this for all of my data for each run to track the bound fraction over time. \n",
    "\n",
    "Then, I'd like to calculate the lagrangian radii for the entire computational domain and track over time and compare between each run and their bound fraction over time. (Figure 7 from Li+19) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt\n",
    "import numpy as np\n",
    "# load turbsph_hdf5 plt file for 2tff\n",
    "ds = yt.load('../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_plt_2133',\n",
    "             particle_filename='../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_part_2133')\n",
    "ad = ds.all_data()\n",
    "print(ds.derived_field_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate energy of each star (KE_s + PE_s + PE_g)\n",
    "star_ind = np.where(ad['particle_type']==1.) # separate star particles from sinks\n",
    "star_kine = 0.5*ad['particle_mass'][star_ind]*(ad['particle_velocity_x'][star_ind]**2 \\\n",
    "                                               + ad['particle_velocity_y'][star_ind]**2 \\\n",
    "                                               + ad['particle_velocity_z'][star_ind]**2)\n",
    "star_ener = star_kine.v+ad['particle_gpot'][star_ind].v*ad['particle_mass'][star_ind].v\n",
    "\n",
    "print(len(np.where(star_ener<0.0)[0]))\n",
    "print(len(np.where(star_ener>0.0)[0]))\n",
    "neg_ener_ind = np.where(star_ener<0.0)[0] # get indices for negative energy stars\n",
    "# note which stars have positive energy\n",
    "star_ener2 = star_kine.v+ad['particle_gpot'][star_ind].v*ad['particle_mass'][star_ind].v\n",
    "print(star_ener2[neg_ener_ind])\n",
    "# recalculate star energies EXCLUDING positive energy stars\n",
    "\n",
    "# repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encountering a little problem\n",
    "\n",
    "What I am able to do now: \n",
    "\n",
    "I can find the stars that have negative total energy due to gas and stars via yt fields. But I cannot iterate this calculation because the only information I have is a single data point from FLASH consitiuting the potential a star feels due to all the gas and all the particles. Therefore, once I make this initial calculation, I am not able to update this data with the new set of particles (now without the Etot>0 stars).\n",
    "\n",
    "I need a different way of going about this where I have more direct control over the gravitational potential data. \n",
    "\n",
    "The best way I can think of right now is to convert the yt gas output into amuse stars and add to AMUSE particle set and use AMUSE functionality to quickly calculate the grav potential exerted by gas on each star.\n",
    "\n",
    "Calculate lagrangian radii with AMUSE functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from amuse.lab import generic_unit_converter, nbody_system\n",
    "from amuse.lab import units as u\n",
    "from amuse.lab import Particles\n",
    "import amuse.lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gas = len(ad['dens'])\n",
    "gas = Particles(num_gas)\n",
    "\n",
    "gas.tag = np.zeros(num_gas)\n",
    "gas.mass = ad['cell_mass'].v | u.g\n",
    "gas.x = ad['x'].v | u.cm\n",
    "gas.y = ad['y'].v | u.cm\n",
    "gas.z = ad['z'].v | u.cm\n",
    "gas.vx = ad['velocity_x'].v | u.cm/u.s\n",
    "gas.vy = ad['velocity_y'].v | u.cm/u.s\n",
    "gas.vz = ad['velocity_z'].v | u.cm/u.s\n",
    "\n",
    "num_stars = len(ad['particle_mass'].v)\n",
    "stars = Particles(num_stars)\n",
    "\n",
    "stars.tag = np.ones(num_stars)\n",
    "stars.mass = ad['particle_mass'].v | u.g\n",
    "stars.x = ad['particle_position_x'].v | u.cm\n",
    "stars.y = ad['particle_position_y'].v | u.cm\n",
    "stars.z = ad['particle_position_z'].v | u.cm\n",
    "stars.vx = ad['particle_velocity_x'].v | u.cm/u.s\n",
    "stars.vy = ad['particle_velocity_y'].v | u.cm/u.s\n",
    "stars.vz = ad['particle_velocity_z'].v | u.cm/u.s\n",
    "\n",
    "\n",
    "superset = Particles()\n",
    "superset.add_particles(gas)\n",
    "print(superset.tag)\n",
    "superset.add_particles(stars)\n",
    "print(superset.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to reduce the computation time. I dont care about the potenital at the points of gas, only the stars. For a single star, the computationally heavy bit is calculating the potential at its location due to all the gas (200k particles). I dont want to have to do this calculation each time I ditch some stars, in fact, I only have to do the gas calculation once, save the potential as a AMUSE attribute in the original stars particle set and then continue with just the stars from there!\n",
    "\n",
    "The calculation of the poential due to the other stars will have to be repeated each time I update the number of stars present, but that will be a comparatively simple operation.\n",
    "\n",
    "I just need to use the indexing functionality of numpy arrays to calculate the potential of only the particles in the superset tagged as stars. Gotta start thinking in broadcasting/vectorization terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_pot_from_gas_and_stars = superset[np.where(superset.tag==1.)].potential().as_quantity_in(u.cm**2 / u.s**2)\n",
    "# add potential energy to stars atrributes\n",
    "stars.potE = star_pot_from_gas_and_stars\n",
    "\n",
    "star_total_E = (stars.specific_kinetic_energy() + stars.potE)*stars.mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbound_ind = np.where(star_total_E.value_in(u.kg*u.m**2/u.s**2) > 0)[0]\n",
    "stars1 = stars.copy()\n",
    "stars1.remove_particles(stars[unbound_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stars.x))\n",
    "print(len(stars1.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superset1 = Particles()\n",
    "superset1.add_particles(gas)\n",
    "superset1.add_particles(stars1)\n",
    "\n",
    "star_pot_from_gas_and_stars = superset1[np.where(superset1.tag==1.)].potential().as_quantity_in(u.cm**2 / u.s**2)\n",
    "# add potential energy to stars atrributes\n",
    "stars1.potE = star_pot_from_gas_and_stars\n",
    "\n",
    "star_total_E = (stars1.specific_kinetic_energy() + stars1.potE)*stars1.mass\n",
    "unbound_ind = np.where(star_total_E.value_in(u.kg*u.m**2/u.s**2) > 0)[0]\n",
    "\n",
    "stars2 = stars1.copy()\n",
    "stars2.remove_particles(stars1[unbound_ind])\n",
    "\n",
    "superset2 = Particles()\n",
    "superset2.add_particles(gas)\n",
    "superset2.add_particles(stars2)\n",
    "star_pot_from_gas_and_stars = superset2[np.where(superset2.tag==1.)].potential().as_quantity_in(u.cm**2 / u.s**2)\n",
    "stars2.potE = star_pot_from_gas_and_stars\n",
    "\n",
    "star_total_E = (stars2.specific_kinetic_energy() + stars2.potE)*stars2.mass\n",
    "unbound_ind = np.where(star_total_E.value_in(u.kg*u.m**2/u.s**2) > 0)[0]\n",
    "\n",
    "stars3 = stars2.copy()\n",
    "stars3.remove_particles(stars2[unbound_ind])\n",
    "\n",
    "superset3 = Particles()\n",
    "superset3.add_particles(gas)\n",
    "superset3.add_particles(stars3)\n",
    "star_pot_from_gas_and_stars = superset3[np.where(superset3.tag==1.)].potential().as_quantity_in(u.cm**2 / u.s**2)\n",
    "stars3.potE = star_pot_from_gas_and_stars\n",
    "\n",
    "star_total_E = (stars3.specific_kinetic_energy() + stars3.potE)*stars3.mass\n",
    "unbound_ind = np.where(star_total_E.value_in(u.kg*u.m**2/u.s**2) > 0)[0]\n",
    "\n",
    "stars4 = stars3.copy()\n",
    "stars4.remove_particles(stars3[unbound_ind])\n",
    "print(len(stars.x))\n",
    "print(len(stars1.x))\n",
    "print(len(stars2.x))\n",
    "print(len(stars3.x))\n",
    "print(len(stars4.x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Iterative technique works!!\n",
    "Great!! It seems like we can remove unbound stars and recalculate the total energy of all stars while taking into account the background gas at the same time.\n",
    "\n",
    "Now let's put this into a single function.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_amuse_set_gas_and_stars(input_hdf5_plt, input_hdf5_part):\n",
    "    \"\"\"\n",
    "    Creates an AMUSE particle set from a FLASH output \n",
    "    containing gas cells and star particles.\n",
    "    Both gas cells and star particles are converted\n",
    "    into AMUSE particles and are catagorized by tag:\n",
    "    gas (0), star (1). The resulting particle set\n",
    "    has attributes of tag, mass, position, and \n",
    "    velocity.\n",
    "    \"\"\"\n",
    "    ds = yt.load('../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_plt_2133',\n",
    "                 particle_filename='../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_part_2133')\n",
    "    ad = ds.all_data()\n",
    "    num_gas = len(ad['dens'])\n",
    "    gas = Particles(num_gas)\n",
    "\n",
    "    gas.tag = np.zeros(num_gas)\n",
    "    gas.mass = ad['cell_mass'].v | u.g\n",
    "    gas.x = ad['x'].v | u.cm\n",
    "    gas.y = ad['y'].v | u.cm\n",
    "    gas.z = ad['z'].v | u.cm\n",
    "    gas.vx = ad['velocity_x'].v | u.cm/u.s\n",
    "    gas.vy = ad['velocity_y'].v | u.cm/u.s\n",
    "    gas.vz = ad['velocity_z'].v | u.cm/u.s\n",
    "\n",
    "    num_stars = len(ad['particle_mass'].v)\n",
    "    stars = Particles(num_stars)\n",
    "\n",
    "    stars.tag = np.ones(num_stars)\n",
    "    stars.mass = ad['particle_mass'].v | u.g\n",
    "    stars.x = ad['particle_position_x'].v | u.cm\n",
    "    stars.y = ad['particle_position_y'].v | u.cm\n",
    "    stars.z = ad['particle_position_z'].v | u.cm\n",
    "    stars.vx = ad['particle_velocity_x'].v | u.cm/u.s\n",
    "    stars.vy = ad['particle_velocity_y'].v | u.cm/u.s\n",
    "    stars.vz = ad['particle_velocity_z'].v | u.cm/u.s\n",
    "\n",
    "    return stars, gas\n",
    "\n",
    "def calculate_star_totE(input_stars, input_gas):\n",
    "    superset = Particles()\n",
    "    superset.add_particles(input_gas)\n",
    "    superset.add_particles(input_stars)\n",
    "    \n",
    "    star_pot_from_gas_and_stars = superset[np.where(superset.tag==1.)].potential().as_quantity_in(u.cm**2 / u.s**2)\n",
    "    input_stars.potE = star_pot_from_gas_and_stars\n",
    "    \n",
    "    return input_stars\n",
    "def remove_stars(input_stars):\n",
    "    unbound = True\n",
    "    star_total_E = (input_stars.specific_kinetic_energy() + input_stars.potE)*input_stars.mass\n",
    "    unbound_ind = np.where(star_total_E.value_in(u.kg*u.m**2/u.s**2) > 0)[0]\n",
    "    tmp_stars = input_stars.copy()\n",
    "    try: \n",
    "        tmp_stars.remove_particles(input_stars[unbound_ind])\n",
    "    except:\n",
    "        unbound=False\n",
    "    return tmp_stars, unbound\n",
    "\n",
    "def remove_unbound_stars(input_stars, input_gas):\n",
    "    superset = Particles()\n",
    "    superset.add_particles(input_gas)\n",
    "    superset.add_particles(input_stars)\n",
    "    \n",
    "    star_pot_from_gas_and_stars = superset[np.where(superset.tag==1.)].potential().as_quantity_in(u.cm**2 / u.s**2)\n",
    "    # add potential energy to stars atrributes\n",
    "    input_stars.potE = star_pot_from_gas_and_stars\n",
    "    \n",
    "    star_total_E = (input_stars.specific_kinetic_energy() + input_stars.potE)*input_stars.mass\n",
    "    input_stars.totE = star_total_E\n",
    "    unbound = True\n",
    "    unbound_ind = np.where(star_total_E.value_in(u.kg*u.m**2/u.s**2) > 0)[0]\n",
    "    try:\n",
    "        if len(unbound_ind[0]) == 0:\n",
    "            unbound = False\n",
    "    except TypeError:\n",
    "        unbound = False\n",
    "    tmp_stars = input_stars.copy()\n",
    "    tmp_stars.remove_particles(input_stars[unbound_ind])\n",
    "    \n",
    "    return tmp_stars, unbound\n",
    "def iterate_star_energy(input_hdf5_plt, input_hdf5_part):\n",
    "    stars, gas = make_amuse_set_gas_and_stars(input_hdf5_plt, input_hdf5_part)\n",
    "    print(len(stars.mass))\n",
    "    unbound = True\n",
    "    while unbound:\n",
    "        stars, unbound = remove_unbound_stars(stars, gas)\n",
    "        print(len(stars.mass))\n",
    "    print(len(stars.mass))\n",
    "    return stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_file_path = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_plt_2133\"\n",
    "part_file_path = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_part_2133\"\n",
    "stars = iterate_star_energy(plt_file_path, part_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(stars.totE.value_in(u.kg*u.m**2/u.s**2)>0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hdf5_plt = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_plt_2133\"\n",
    "input_hdf5_part = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_part_2133\"\n",
    "stars, gas = make_amuse_set_gas_and_stars(input_hdf5_plt, input_hdf5_part)\n",
    "unbound = True\n",
    "while unbound:\n",
    "    stars_ = calculate_star_totE(stars,gas)\n",
    "    stars_, unbound = remove_stars(stars)\n",
    "    print(len(stars_))\n",
    "print(len(stars_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having some trouble\n",
    "This whole \"combining things into one function\" business is really tripping me up. \n",
    "\n",
    "**The problem:** For the L50M run, the remove bound-stars iteration should cycle several times (I checked this by brute force), but instead it removes stars from the domain once and then determines it is completed (no other stars have >0 Etot) and it stops. \n",
    "\n",
    "**Why is this happening:** It's clear that there's something wrong with how the AMUSE particle sets are being updated and reinitialized. \n",
    "\n",
    "**How to fix:** At the moment I've been concatenating the gas and stars into a superset AMUSE particle set, then calculating energy for the stars, saving the result to the input star particles not the superset, then copying the input star particle set, removing the star particles with an Etot of > 0, and passing out this copy of the stars. This updated copy is then passed back into the beginning of this process.\n",
    "\n",
    "Now let's try this without all the copying, passing copies back into the iteration, etc. Let's just make a superset once an manipulate everything within the superset.\n",
    "\n",
    "## IT WORKS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt\n",
    "yt.mylog.level=50\n",
    "import numpy as np\n",
    "from amuse.lab import generic_unit_converter, nbody_system\n",
    "from amuse.lab import units as u\n",
    "from amuse.lab import Particles\n",
    "import amuse.lab\n",
    "from time import time\n",
    "def make_amuse_set_gas_and_stars(input_hdf5_plt, input_hdf5_part, calc_gas_ener=False):\n",
    "    \"\"\"\n",
    "    Creates an AMUSE particle set from a FLASH output \n",
    "    containing gas cells and star particles.\n",
    "    Both gas cells and star particles are converted\n",
    "    into AMUSE particles and are catagorized by tag:\n",
    "    gas (0), star (1). The resulting particle set\n",
    "    has attributes of tag, mass, position, and \n",
    "    velocity.\n",
    "    \n",
    "    Arguments:\n",
    "    input_hdf5_plt  - hdf5 plot file path\n",
    "    input_hdf5_part - hdf5 particle file path\n",
    "    calc_gas_ener   - flag to set particle magnetic and internal energy, \n",
    "                      used to calculate gas total energy to remove unbound gas.\n",
    "                      Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    stars - star particle set\n",
    "    gas   - gas particle set\n",
    "    \"\"\"\n",
    "    # Load hdf5 files in yt so we can extract cell and particle information\n",
    "    ds = yt.load(input_hdf5_plt,\n",
    "                 particle_filename=input_hdf5_part)\n",
    "    ad = ds.all_data()\n",
    "    \n",
    "    # Set gas cell properties as AMUSE particle data\n",
    "    num_gas = len(ad['dens'])\n",
    "    gas = Particles(num_gas)\n",
    "    \n",
    "    gas.tag = np.zeros(num_gas)\n",
    "    gas.mass = ad['cell_mass'].v | u.g\n",
    "    gas.x = ad['x'].v | u.cm\n",
    "    gas.y = ad['y'].v | u.cm\n",
    "    gas.z = ad['z'].v | u.cm\n",
    "    gas.vx = ad['velocity_x'].v | u.cm/u.s\n",
    "    gas.vy = ad['velocity_y'].v | u.cm/u.s\n",
    "    gas.vz = ad['velocity_z'].v | u.cm/u.s\n",
    "    if (calc_gas_ener):\n",
    "        # Set gas thermal and magnetic energy\n",
    "        gas.ME = ad['magp'].v*ad['cell_volume'].v / 4.0 / np.pi | u.erg\n",
    "        gas.EI = ad['cell_mass'].v * ad['eint'].v | u.erg\n",
    "        \n",
    "    # Set star particle properties as AMUSE particle data\n",
    "    # (must be same labels for when we concatenate with gas)\n",
    "    num_stars = len(ad['particle_mass'].v)\n",
    "    stars = Particles(num_stars)\n",
    "\n",
    "    stars.tag = np.ones(num_stars)\n",
    "    stars.mass = ad['particle_mass'].v | u.g\n",
    "    stars.x = ad['particle_position_x'].v | u.cm\n",
    "    stars.y = ad['particle_position_y'].v | u.cm\n",
    "    stars.z = ad['particle_position_z'].v | u.cm\n",
    "    stars.vx = ad['particle_velocity_x'].v | u.cm/u.s\n",
    "    stars.vy = ad['particle_velocity_y'].v | u.cm/u.s\n",
    "    stars.vz = ad['particle_velocity_z'].v | u.cm/u.s\n",
    "    if (calc_gas_ener):\n",
    "        # Set dummy magnetic and internal energy\n",
    "        stars.ME = np.zeros(num_stars) | u.erg\n",
    "        stars.EI = np.zeros(num_stars) | u.erg\n",
    "\n",
    "    return stars, gas\n",
    "\n",
    "def setup_superset(input_hdf5_plt, input_hdf5_part, calc_gas_ener=False):\n",
    "    '''\n",
    "    Generates an AMUSE superset of gas and star particles\n",
    "    tags = [0] and [1] respectively.\n",
    "    Gas and star particles have positions, velocities, and masses.\n",
    "    \n",
    "    We then calculate the total energy of each star particle\n",
    "    based on its KE, and PE due to all other stars and gas\n",
    "    using update_total_E(superset).\n",
    "    \n",
    "    Arguments:\n",
    "    input_hdf5_plt  - hdf5 plot file path\n",
    "    input_hdf5_part - hdf5 particle file path\n",
    "    calc_gas_ener   - flag to set particle magnetic and internal energy, \n",
    "                      used to calculate gas total energy to remove unbound gas.\n",
    "                      Default is False.\n",
    "    \n",
    "    Other Function Calls:\n",
    "    update_total_E(superset,calc_gas_ener)\n",
    "    \n",
    "    Returns:\n",
    "    superset - star and gas particles\n",
    "    '''\n",
    "    # First extract star and gas positions, velocities, and masses\n",
    "    # from hdf5 output using yt.\n",
    "    stars, gas = make_amuse_set_gas_and_stars(input_hdf5_plt, input_hdf5_part, calc_gas_ener)\n",
    "    \n",
    "    # Build superset.\n",
    "    superset = Particles()\n",
    "    superset.add_particles(gas)\n",
    "    superset.add_particles(stars)\n",
    "    # Calculate and set total energy particle attribute\n",
    "    superset = update_total_E(superset, calc_gas_ener)\n",
    "    return superset\n",
    "\n",
    "def update_total_E(superset,calc_gas_ener=False):\n",
    "    '''\n",
    "    Calculates the Total Energy of all the particles \n",
    "    tagged as [1] (star) in an AMUSE superset. We do some \n",
    "    casual array indexing to ensure the potential calculation\n",
    "    for the star particles includes the masses of all the gas,\n",
    "    but does not calculate the potential for each gas particle.\n",
    "    \n",
    "    We then update the totE quantity of the superset, setting\n",
    "    the totE for the stars as our calculated values and the \n",
    "    totE for the gas as -1 (since we dont need this value and\n",
    "    this ensures the gas is not removed from the superset in\n",
    "    upcoming steps).\n",
    "    \n",
    "    Arguments:\n",
    "    superset - AMSUE superset of gas and star particles\n",
    "    calc_gas_ener   - flag to set particle magnetic and internal energy, \n",
    "                      used to calculate gas total energy to remove unbound gas.\n",
    "                      Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    superset - superset with updated totE values\n",
    "    '''\n",
    "    # Calculate total energy of star particles only\n",
    "    # starKE + PE_stars + PE_gas\n",
    "    star_ind = np.where(superset.tag==1.)\n",
    "    star_pot_from_gas_and_stars = superset[star_ind].potential().as_quantity_in(u.cm**2 / u.s**2)\n",
    "    star_spec_kine = superset[star_ind].specific_kinetic_energy().as_quantity_in(u.cm**2 / u.s**2)\n",
    "    star_mass = superset[star_ind].mass.as_quantity_in(u.g)\n",
    "    star_totE = star_mass*(star_spec_kine+star_pot_from_gas_and_stars)\n",
    "    \n",
    "    gas_ind = np.where(superset.tag==0.)\n",
    "    if (calc_gas_ener):\n",
    "        # Calculate gas total energy\n",
    "        print(\"Calculating gas potential...\") # Printing to screen b/c this step can take a while\n",
    "        gas_pot_from_gas_and_stars = superset[gas_ind].potential().as_quantity_in(u.cm**2 / u.s**2)\n",
    "        gas_spec_kine = superset[gas_ind].specific_kinetic_energy().as_quantity_in(u.cm**2 / u.s**2)\n",
    "        gas_mass = superset[gas_ind].mass.as_quantity_in(u.g)\n",
    "        gas_ME = superset[gas_ind].ME.as_quantity_in(u.J)\n",
    "        gas_EI = superset[gas_ind].EI.as_quantity_in(u.J)\n",
    "        gas_totE = gas_mass*(gas_spec_kine+gas_pot_from_gas_and_stars)+gas_ME+gas_EI\n",
    "    else:\n",
    "        # Set dummy totE attribute to gas particles\n",
    "        gas_totE = (-1 | u.J)*np.ones(len(superset[gas_ind]))\n",
    "    # Set superset totE attribute as concatenated gas and star totE arrays.\n",
    "    superset.totE = np.concatenate((gas_totE.as_quantity_in(u.J),star_totE.as_quantity_in(u.J)))\n",
    "    return superset\n",
    "def iterate_remove_stars(superset, iterate_gas_ener=False, report_removal_stats=False):\n",
    "    '''\n",
    "    Iteration following the steps:\n",
    "    1. Remove star particles from superset with total E > 0.\n",
    "    2. Check if we did in fact remove stars, if No then set break bool.\n",
    "    3. Update star total energies using updated superset particles.\n",
    "    \n",
    "    Arguments:\n",
    "    superset             - AMUSE particle superset of gas and star particles\n",
    "    iterate_gas_ener     - Flag to iteratively remove unbound gas.\n",
    "                           Only set to True in conjunction with calc_gas_ener.\n",
    "                           Setting to True is not recommended for refined \n",
    "                           simulations as the operation is very expensive.\n",
    "                           Default is False.\n",
    "    report_removal_stats - Report number of stars removed, \n",
    "                           total mass of removed stars.\n",
    "    \n",
    "    Other Function Calls:\n",
    "    update_total_E(superset, calc_gas_ener)\n",
    "    \n",
    "    Returns:\n",
    "    superset - Final superset in which no stars with total E > 0 exist\n",
    "    '''\n",
    "    supersetf = superset.copy()\n",
    "    unbound = True\n",
    "    if (report_removal_stats):\n",
    "        print(\"Reporting removal stats!\")\n",
    "        start_num_stars = len(superset[np.where(superset.tag==1.)])\n",
    "        start_mass_stars = superset[np.where(superset.tag==1.)].mass.sum()\n",
    "        print(\"Initial num stars:\", start_num_stars)\n",
    "        print(\"Initial mass stars: {:.2f} MSun\".format(\\\n",
    "                    start_mass_stars.value_in(u.MSun)))\n",
    "        print(\"---------------------\")\n",
    "    if (iterate_gas_ener):\n",
    "        print(\"Removing unbound gas iteratively. \\\n",
    "        Strap in, this will take a while...\")\n",
    "    while(unbound):\n",
    "        if (report_removal_stats):\n",
    "            print(\"\\nUnbound stars detected...\")\n",
    "            init_num_stars = len(supersetf[np.where(supersetf.tag==1.)])\n",
    "            init_mass_stars = supersetf[np.where(supersetf.tag==1.)].mass.sum()\n",
    "        unbound_ind = np.where(supersetf.totE.value_in(u.J) > 0)[0]\n",
    "        supersetf.remove_particles(supersetf[unbound_ind])\n",
    "        \n",
    "        if (report_removal_stats): \n",
    "            final_num_stars = len(supersetf[np.where(supersetf.tag==1.)])\n",
    "            final_mass_stars = supersetf[np.where(supersetf.tag==1.)].mass.sum()\n",
    "            print(\"num stars removed:\", init_num_stars-final_num_stars)\n",
    "            print(\"mass stars rmvd: {:.2f} MSun\".format(\\\n",
    "                    (init_mass_stars-final_mass_stars).value_in(u.MSun)))\n",
    "            \n",
    "        if len(unbound_ind) == 0:\n",
    "            unbound = False\n",
    "        supersetf = update_total_E(supersetf, iterate_gas_ener)\n",
    "        \n",
    "    if (report_removal_stats):\n",
    "        end_num_stars = len(supersetf[np.where(supersetf.tag==1.)])\n",
    "        end_mass_stars = supersetf[np.where(supersetf.tag==1.)].mass.sum()\n",
    "        print(\"\\nSummary: \\ntotal stars removed:\", start_num_stars-end_num_stars)\n",
    "        print(\"total star mass rmvd: {:.2f} MSun\".format(\\\n",
    "                    (start_mass_stars-end_mass_stars).value_in(u.MSun)))\n",
    "        print(\"total mass rmvd: {:.2f} MSun\".format(\\\n",
    "                    (superset.mass.sum()-supersetf.mass.sum()).value_in(u.MSun)))\n",
    "    return supersetf\n",
    "\n",
    "def save_orig_and_iterated_psets(orig_pset, orig_file_name, iter_pset, iter_file_name):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting removal stats!\n",
      "Initial num stars: 2157\n",
      "Initial mass stars: 1503.04 MSun\n",
      "---------------------\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 415\n",
      "mass stars rmvd: 329.34 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 126\n",
      "mass stars rmvd: 53.92 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 23\n",
      "mass stars rmvd: 11.89 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 7\n",
      "mass stars rmvd: 2.09 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 3\n",
      "mass stars rmvd: 1.37 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 1\n",
      "mass stars rmvd: 0.32 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 0\n",
      "mass stars rmvd: 0.00 MSun\n",
      "\n",
      "Summary: \n",
      "total stars removed: 575\n",
      "total star mass rmvd: 398.92 MSun\n",
      "total mass rmvd: 398.92 MSun\n"
     ]
    }
   ],
   "source": [
    "L50M_plt = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_plt_2133\"\n",
    "L50M_part = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_part_2133\"\n",
    "L50M_superset = setup_superset(L50M_plt, L50M_part, calc_gas_ener=False)\n",
    "\n",
    "L50M_fsuperset = iterate_remove_stars(L50M_superset, iterate_gas_ener=False, report_removal_stats=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting removal stats!\n",
      "Initial num stars: 1586\n",
      "Initial mass stars: 998.61 MSun\n",
      "---------------------\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 664\n",
      "mass stars rmvd: 384.37 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 248\n",
      "mass stars rmvd: 132.77 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 113\n",
      "mass stars rmvd: 57.23 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 48\n",
      "mass stars rmvd: 29.41 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 32\n",
      "mass stars rmvd: 12.70 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 9\n",
      "mass stars rmvd: 4.31 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 4\n",
      "mass stars rmvd: 1.58 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 0\n",
      "mass stars rmvd: 0.00 MSun\n",
      "\n",
      "Summary: \n",
      "total stars removed: 1118\n",
      "total star mass rmvd: 622.36 MSun\n",
      "total mass rmvd: 622.36 MSun\n"
     ]
    }
   ],
   "source": [
    "L70M_plt = \"../Torch-Analysis/Paper1-runs-snapshots/L3-70M/2tff_hdf5_plt_1334\"\n",
    "L70M_part = \"../Torch-Analysis/Paper1-runs-snapshots/L3-70M/2tff_hdf5_part_1334\"\n",
    "\n",
    "L70M_superset = setup_superset(L70M_plt, L70M_part, calc_gas_ener=False)\n",
    "\n",
    "L70M_fsuperset = iterate_remove_stars(L70M_superset, iterate_gas_ener=False, report_removal_stats=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting removal stats!\n",
      "Initial num stars: 1389\n",
      "Initial mass stars: 893.53 MSun\n",
      "---------------------\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 558\n",
      "mass stars rmvd: 354.70 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 137\n",
      "mass stars rmvd: 84.32 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 67\n",
      "mass stars rmvd: 31.57 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 31\n",
      "mass stars rmvd: 11.79 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 18\n",
      "mass stars rmvd: 7.67 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 9\n",
      "mass stars rmvd: 8.74 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 8\n",
      "mass stars rmvd: 3.82 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 6\n",
      "mass stars rmvd: 5.60 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 7\n",
      "mass stars rmvd: 2.81 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 4\n",
      "mass stars rmvd: 0.77 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 1\n",
      "mass stars rmvd: 2.48 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 1\n",
      "mass stars rmvd: 0.15 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 0\n",
      "mass stars rmvd: 0.00 MSun\n",
      "\n",
      "Summary: \n",
      "total stars removed: 847\n",
      "total star mass rmvd: 514.40 MSun\n",
      "total mass rmvd: 514.40 MSun\n"
     ]
    }
   ],
   "source": [
    "L100M_plt = \"../Torch-Analysis/Paper1-runs-snapshots/L3-100M/2tff_hdf5_plt_1981\"\n",
    "L100M_part = \"../Torch-Analysis/Paper1-runs-snapshots/L3-100M/2tff_hdf5_part_1981\"\n",
    "\n",
    "L100M_superset = setup_superset(L100M_plt, L100M_part, calc_gas_ener=False)\n",
    "L100M_fsuperset = iterate_remove_stars(L100M_superset, iterate_gas_ener=False, report_removal_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting removal stats!\n",
      "Initial num stars: 6305\n",
      "Initial mass stars: 3603.56 MSun\n",
      "---------------------\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 182\n",
      "mass stars rmvd: 122.10 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 11\n",
      "mass stars rmvd: 3.50 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 1\n",
      "mass stars rmvd: 0.10 MSun\n",
      "\n",
      "Unbound stars detected...\n",
      "num stars removed: 0\n",
      "mass stars rmvd: 0.00 MSun\n",
      "\n",
      "Summary: \n",
      "total stars removed: 194\n",
      "total star mass rmvd: 125.71 MSun\n",
      "total mass rmvd: 125.71 MSun\n"
     ]
    }
   ],
   "source": [
    "Lv_plt = \"../Torch-Analysis/Paper1-runs-snapshots/L3-v/2tff_hdf5_plt_2070\"\n",
    "Lv_part = \"../Torch-Analysis/Paper1-runs-snapshots/L3-v/2tff_hdf5_part_2070\"\n",
    "\n",
    "Lv_superset = setup_superset(Lv_plt, Lv_part, calc_gas_ener=False)\n",
    "Lv_fsuperset = iterate_remove_stars(Lv_superset, iterate_gas_ener=False, report_removal_stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NExt up\n",
    "\n",
    "I want the ability to remove all of the GAS with positive energy too. The main issues with this is it's very computationally expensive (need to calculate potential on 300k+ particles due to 300k+ particles). For now, I can just take the compuational hit once, remove the unbound gas at the beginning and then just not iterate on it. I can also embed this in the iterate or setup_superset functions as an optional task (create remove_gas=True kwarg)\n",
    "* unbound gas can now be removed. Doing so takes ~30min to calculate the potential at each gas cell.\n",
    "    * for 50M, this removes 2000Msun of gas, leaving 3000Msun\n",
    "* Unbound gas removal can also now be iterated on, but I can't imagine using it as it would take hours to cylce through.\n",
    "\n",
    "Questions I want to answer:\n",
    "\n",
    "1. What do the remaining stars look like? What is their mass and spatial distributions?\n",
    "2. What do the removed stars look like? What is their mass and spatial distributions?\n",
    "3. What are the Lagrangiian radii of remaining stars\n",
    "4. Does DBSCAN pick up diffrerent cluster in remaining stars?\n",
    "5. What do KS-tests look like for remaining stars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L50M_plt = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_plt_2133\"\n",
    "L50M_part = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_part_2133\"\n",
    "tick = time()\n",
    "L50M_superset = setup_superset(L50M_plt, L50M_part, calc_gas_ener=True)\n",
    "tock = time()\n",
    "print(\"time setup (s):\", tock-tick)\n",
    "tick=time()\n",
    "L50M_fsuperset = iterate_remove_stars(L50M_superset, iterate_gas_ener=False)\n",
    "tock=time()\n",
    "print(len(L50M_fsuperset[np.where(L50M_fsuperset.tag==1.)]))\n",
    "print(\"time iterate remove: (s)\", tock-tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L50M_plt = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_plt_2133\"\n",
    "L50M_part = \"../Torch-Analysis/Paper1-runs-snapshots/L3-50M/2tff_hdf5_part_2133\"\n",
    "L50M_superset = setup_superset(L50M_plt, L50M_part, calc_gas_ener=False)\n",
    "print(L50M_superset[np.where(L50M_superset.tag==0.)].mass.sum().value_in(u.MSun))\n",
    "print(L50M_fsuperset[np.where(L50M_fsuperset.tag==0.)].mass.sum().value_in(u.MSun))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
